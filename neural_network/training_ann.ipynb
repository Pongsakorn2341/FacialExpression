{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cwd = os.getcwd()\n",
    "df = pd.read_csv(cwd + \"\\\\..\\\\data_csv\\\\preprocessing_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract raw_images and labels\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "count = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "raw_images =  []\n",
    "labels = []\n",
    "for i, row in df.iterrows():\n",
    "    if count[row.emotion] > 3000:\n",
    "        continue\n",
    "\n",
    "    image_path = row.image\n",
    "    full_image_path = os.path.dirname(cwd) + \"\\\\cleaned_images\\\\\" + image_path\n",
    "    image = cv2.imread(full_image_path)\n",
    "    pixels = image.flatten()\n",
    "    raw_images.append(pixels)\n",
    "    \n",
    "    label = row.emotion\n",
    "    labels.append(label)\n",
    "    \n",
    "    count[row.emotion] += 1\n",
    "    if i > 0 and i % 1000 == 0: print('[INFO] processed {}/{}'.format(i, len(df)))\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hypoparameter\n",
    "\n",
    "from hyperopt.pyll.base import scope \n",
    "from hyperopt import hp\n",
    "#quniform returns float, some parameters require int; use this to force int\n",
    "space = {\n",
    "         'rate'       : hp.uniform('rate', 0.01, 0.5),\n",
    "         'dropout'    : hp.uniform('dropout', 0.01, 0.5),\n",
    "         'units1'      : scope.int(hp.quniform('units1', 10, 100, 5)),\n",
    "         'units2'      : scope.int(hp.quniform('units2', 10, 100, 5)),\n",
    "         'units3'      : scope.int(hp.quniform('units3', 10, 100, 5)),\n",
    "         'units4'      : scope.int(hp.quniform('units4', 10, 100, 5)),\n",
    "         'batch_size' : scope.int(hp.quniform('batch_size', 100, 250, 25)),\n",
    "         'layers'     : scope.int(hp.quniform('layers', 3, 5, 1)),\n",
    "         'optimizer'  : hp.choice('optimizer', ['adam', 'adadelta', 'sgd', 'RMSprop']),\n",
    "         'epochs'     : scope.int(hp.quniform('epochs', 100, 500, 10)),\n",
    "         'activation' : hp.choice('activation', ['relu', 'sigmoid', 'tanh', 'elu']),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_nn(params):\n",
    "    print(\"params\", params)\n",
    "\n",
    "    # Keras LSTM model\n",
    "    model = Sequential()\n",
    "    \n",
    "    if params['layers'] == 1:\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "    else:\n",
    "        # First layer specifies input_shape and returns sequences\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "        \n",
    "        # Middle layers return sequences\n",
    "        for i in range(params['layers']-2):\n",
    "            model.add(Dense(params['units' + str(i + 2)], activation=params['activation']))\n",
    "            #model.add(Dropout(rate=params['rate']))\n",
    "        \n",
    "        # Last layer doesn't return anything\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "        model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.compile(optimizer=params['optimizer'], loss='mean_squared_error')\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss',mode='min', verbose=1,patience=15)\n",
    "    '''result = model.fit(X_train, y_train, \n",
    "                       verbose=0, \n",
    "                       validation_split=0.1,\n",
    "                       batch_size=params['batch_size'],\n",
    "                       epochs=200)'''\n",
    "    result =  model.fit(X_train, y_train, validation_data=(X_val, y_val,), batch_size=params['batch_size'], epochs=params[\"epochs\"], verbose=0)\n",
    "    \n",
    "    # Get the lowest validation loss of the training epochs\n",
    "    validation_loss = np.amin(result.history['val_loss']) \n",
    "    print('Best validation loss of epoch:', validation_loss)\n",
    "    \n",
    "    return {'loss': validation_loss, \n",
    "            'status': STATUS_OK, \n",
    "            'model': model, \n",
    "            'params': params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for hypoparameter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(raw_images), np.array(labels), test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_val = le.transform(y_val)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from keras.callbacks import EarlyStopping\n",
    "trials = Trials()\n",
    "best = fmin(f_nn, \n",
    "            space, \n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
    "best_params = trials.results[np.argmin([r['loss'] for r in trials.results])]['params']\n",
    "worst_model = trials.results[np.argmax([r['loss'] for r in trials.results])]['model']\n",
    "worst_params = trials.results[np.argmax([r['loss'] for r in trials.results])]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defind model for plot roc curve and test model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def classification_model_multi():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# no need to use\n",
    "def classification_model_bin():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for plot roc curve\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(raw_images), np.array(labels), test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to use but keep learning to_categorical()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_val = le.transform(y_val)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "dictionary = ['ANGER', 'CONTEMPT', 'DISGUST', 'FEAR', 'HAPPINESS',  'NEUTRAL', 'SADNESS', 'SURPRISE']\n",
    "\n",
    "for emo in range(8):\n",
    "    print(dictionary[emo])\n",
    "\n",
    "    emo_feature = []\n",
    "    emo_target = []\n",
    "\n",
    "    for i in range(len(df['emotion'])):\n",
    "        if df['emotion'][i] == emo:\n",
    "            emo_target.append(1)\n",
    "        else:\n",
    "            emo_target.append(0)\n",
    "        \n",
    "        image_path = row.image\n",
    "        full_image_path = os.path.dirname(cwd) + \"\\\\cleaned_images\\\\\" + image_path\n",
    "        image = cv2.imread(full_image_path)\n",
    "        pixels = image.flatten()\n",
    "        emo_feature.append(pixels)\n",
    "\n",
    "    emo_feature = np.array(emo_feature)\n",
    "    emo_target = np.array(emo_target)\n",
    "\n",
    "    # import data\n",
    "    X = emo_feature\n",
    "    y = emo_target\n",
    "\n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "    history = []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    for i, (train, val) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_train, X_val = X[train], X[val]\n",
    "        y_train, y_val = y[train], y[val]\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        y_train = to_categorical(y_train)\n",
    "        y_val = to_categorical(y_val)\n",
    "\n",
    "        # create and fit model\n",
    "        model = classification_model_multi()\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val,), epochs=100, verbose=2)\n",
    "\n",
    "        # predict\n",
    "        y_pred = model.predict(X_val).ravel()\n",
    "        y_val = y_val.ravel()\n",
    "        \n",
    "        print(f'====================Fold {i}====================', '\\n')\n",
    "\n",
    "        # plot ROC curve\n",
    "        viz = RocCurveDisplay.from_predictions(y_val, y_pred, ax=ax, name=\"ROC fold {}\".format(i), alpha=0.3, lw=1,)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "        \n",
    "    # middle line\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    # mean line\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    # std\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(\n",
    "        mean_fpr,\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05],\n",
    "            ylim=[-0.05, 1.05],\n",
    "            title=\"Receiver operating characteristic\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.savefig(cwd + '/../graph/' + dictionary[emo] + '/ann.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for test \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(raw_images), np.array(labels), test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_val = le.transform(y_val)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classification_model_multi()\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val,), epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT CONFUSION MATRIX\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dictionary)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "disp.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = 'model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "value = model.predict(X_test)\n",
    "y_pred = np.argmax(value, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = model.predict(X_test)\n",
    "y_pred = np.argmax(value,axis=1)\n",
    "y_true = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_arr = list(y_pred)\n",
    "print(\"len : \", len(y_pred_arr))\n",
    "score = 0\n",
    "for i in range(len(y_pred_arr)):\n",
    "    if(list(y_pred)[i] == list(y_true)[i]): score += 1\n",
    "max_score = len(list(y_pred))\n",
    "print(f\"Final Score : {score}/{max_score}\")\n",
    "print(\"Accuracy : \", 100 * score / max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Process #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint\n",
    "# Parameters and time for FOPDT model\n",
    "ns = 10000\n",
    "t = np.linspace(0,ns-1,ns)\n",
    "u = np.zeros(ns)\n",
    "# Additional FOPDT parameters\n",
    "yp0 = 0.0\n",
    "u0 = u[0]\n",
    "Km = 0.67\n",
    "taum = 160.0\n",
    "def fopdt(y,t,um,Km,taum):\n",
    "    # arguments\n",
    "    #  y      = output\n",
    "    #  t      = time\n",
    "    #  uf     = input linear function (for time shift)\n",
    "    #  Km     = model gain\n",
    "    #  taum   = model time constant\n",
    "    # calculate derivative\n",
    "    dydt = (-(y-yp0) + Km * (um-u0))/taum\n",
    "    return dydt\n",
    "\n",
    "def sim_model(Km,taum):\n",
    "    # array for model values\n",
    "    ym = np.zeros(ns)\n",
    "    # initial condition\n",
    "    ym[0] = yp0\n",
    "    # loop through time steps    \n",
    "    for i in range(0,ns-1):\n",
    "        ts = [t[i],t[i+1]]\n",
    "        y1 = odeint(fopdt,ym[i],ts,args=(u[i],Km,taum))\n",
    "        ym[i+1] = y1[-1]\n",
    "    return ym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "end = 60 # leave 1st minute of u as 0\n",
    "while end <= ns:\n",
    "    start = end\n",
    "    end += random.randint(300,900) # keep new Q1s value for anywhere from 5 to 15 minutes\n",
    "    u[start:end] = random.randint(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate FOPDT model\n",
    "y = sim_model(Km,taum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Gaussian noise\n",
    "noise = np.random.normal(0,0.2,ns)\n",
    "y += noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Scale data\n",
    "data = np.vstack((u,y)).T\n",
    "s = MinMaxScaler(feature_range=(0,1))\n",
    "data_s = s.fit_transform(data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
