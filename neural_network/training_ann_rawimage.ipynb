{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import pandas as pd, numpy as np\n",
    "import cv2, imutils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from hyperopt.pyll.base import scope \n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "pre_csv = os.path.abspath(os.path.join(os.sep, cwd, '..', 'neural_network', 'augmentation_data.csv'))\n",
    "df = pd.read_csv(pre_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFullImagePath(full_path):\n",
    "    sub_image_path = full_path\n",
    "    split_sub_image_path = sub_image_path.split(\"/\")\n",
    "    image_path = \"\"\n",
    "    if(len(split_sub_image_path) == 2):\n",
    "        image_path = os.path.abspath(os.path.join(os.sep, cwd, \"..\", \"neural_network\", \"augmentation\", split_sub_image_path[0], split_sub_image_path[1]))\n",
    "    else:\n",
    "        image_path = os.path.abspath(os.path.join(os.sep, cwd, \"..\", \"neural_network\", \"augmentation\", sub_image_path))\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/77205\n",
      "[INFO] processed 20000/77205\n",
      "[INFO] processed 30000/77205\n",
      "[INFO] processed 40000/77205\n",
      "[INFO] processed 50000/77205\n",
      "[INFO] processed 60000/77205\n",
      "[INFO] processed 70000/77205\n",
      "[9948, 8812, 9692, 10002, 10001, 10001, 10001, 8748]\n"
     ]
    }
   ],
   "source": [
    "count = [0 for x in range(8)]\n",
    "raw_images =  []\n",
    "labels = []\n",
    "for i, row in df.iterrows():\n",
    "    image_path = getFullImagePath(row.image)\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    pixels = image.flatten()\n",
    "    raw_images.append(pixels)\n",
    "    label = row.emotion\n",
    "    labels.append(label)\n",
    "    \n",
    "    count[row.emotion] += 1\n",
    "    if i > 0 and i % 10000 == 0: print('[INFO] processed {}/{}'.format(i, len(df)))\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'rate'       : hp.uniform('rate', 0.01, 0.5),\n",
    "    'dropout'    : hp.uniform('dropout', 0.01, 0.5),\n",
    "    'units1'      : scope.int(hp.quniform('units1', 10, 100, 5)),\n",
    "    'units2'      : scope.int(hp.quniform('units2', 10, 100, 5)),\n",
    "    'units3'      : scope.int(hp.quniform('units3', 10, 100, 5)),\n",
    "    'units4'      : scope.int(hp.quniform('units4', 10, 100, 5)),\n",
    "    'batch_size' : scope.int(hp.quniform('batch_size', 100, 250, 25)),\n",
    "    'layers'     : scope.int(hp.quniform('layers', 3, 4, 1)),\n",
    "    'optimizer'  : hp.choice('optimizer', ['adam', 'adadelta', 'sgd', 'RMSprop']),\n",
    "    'epochs'     : scope.int(hp.quniform('epochs', 100, 300, 10)),\n",
    "    'activation' : hp.choice('activation', ['relu']), # ['relu', 'sigmoid', 'tanh', 'elu']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_nn(params):\n",
    "    print(\"params\", params)\n",
    "\n",
    "    # Keras LSTM model\n",
    "    model = Sequential()\n",
    "\n",
    "    # params['layers'] => num of hidden layer\n",
    "\n",
    "    if params['layers'] == 1:\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "    else:\n",
    "        # First layer specifies input_shape and returns sequences\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Middle layers return sequences\n",
    "        for i in range(params['layers']-2):\n",
    "            model.add(Dense(params['units' + str(i + 2)], activation=params['activation']))\n",
    "            #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Last layer doesn't return anything                                            \n",
    "        model.add(Dense(params['units' + str(params['layers'])], activation=params['activation']))\n",
    "        model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.compile(optimizer=params['optimizer'], loss='mean_squared_error', metrixs=['accuracy'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss',mode='min', verbose=1,patience=15)\n",
    "    '''result = model.fit(X_train, y_train, \n",
    "                       verbose=0, \n",
    "                       validation_split=0.1,\n",
    "                       batch_size=params['batch_size'],\n",
    "                       epochs=200)'''\n",
    "    result =  model.fit(X_train, y_train, validation_data=(X_val, y_val,), batch_size=params['batch_size'], epochs=params[\"epochs\"], verbose=0)\n",
    "\n",
    "    # Get the lowest validation loss of the training epochs\n",
    "    validation_loss = np.amin(result.history['val_loss']) \n",
    "    print('Best validation loss of epoch:', validation_loss)\n",
    "\n",
    "    return {'loss': validation_loss, \n",
    "            'status': STATUS_OK, \n",
    "            'model': model, \n",
    "            'params': params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(raw_images), np.array(labels), test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "print(\"y_train bf trans_form : \", y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_val = le.transform(y_val)\n",
    "print(\"y_train af trans_form : \", y_train)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)\n",
    "print(\"y_train af to_categorical : \", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Hyperparameter\n",
    "trials = Trials()\n",
    "best = fmin(f_nn, \n",
    "            space, \n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
    "best_params = trials.results[np.argmin([r['loss'] for r in trials.results])]['params']\n",
    "worst_model = trials.results[np.argmax([r['loss'] for r in trials.results])]['model']\n",
    "worst_params = trials.results[np.argmax([r['loss'] for r in trials.results])]['params']\n",
    "print(best_model)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation = 'relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation=activation, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation=activation))\n",
    "    model.add(Dense(16, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_best_param(params):\n",
    "    model = Sequential()\n",
    "\n",
    "    if params['layers'] == 1:\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "    else:\n",
    "        # First layer specifies input_shape and returns sequences\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Middle layers return sequences\n",
    "        for i in range(params['layers']-2):\n",
    "            model.add(Dense(params['units' + str(i + 2)], activation=params['activation']))\n",
    "            #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Last layer doesn't return anything                                            \n",
    "        model.add(Dense(params['units' + str(params['layers'])], activation=params['activation']))\n",
    "        model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.compile(optimizer=params['optimizer'], loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = {'activation': 'relu', 'batch_size': 125, 'dropout': 0.022269352793562694, 'epochs': 190, 'layers': 4, 'optimizer': 'adadelta', 'rate': 0.341066733379372, 'units1': 70, 'units2': 55, 'units3': 40, 'units4': 25}\n",
    "\n",
    "model = create_model_best_param(best_params)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val,), batch_size=best_params['batch_size'], epochs=best_params[\"epochs\"], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "activation = \"relu\"\n",
    "optimizer = \"adam\"\n",
    "my_model = create_model(activation)\n",
    "print(my_model.summary())\n",
    "my_model.fit(X_train, y_train, validation_data=(X_val, y_val,), epochs=100, verbose=2)\n",
    "print(my_model.summary())\n",
    "print (\"-\" * 25, \" Best Params \", \"-\" * 25)\n",
    "best_param_model = create_model_best_param(best_params)\n",
    "print(best_param_model.summary())\n",
    "best_param_model.fit(X_train, y_train, validation_data=(X_val, y_val,), epochs=100, verbose=2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle\n",
    "\n",
    "filename_best = f\"model_{activation}_{optimizer}_best.sav\"\n",
    "filename = f\"model_{activation}_{optimizer}.sav\"\n",
    "# Export 2 model (best, simple)\n",
    "pickle.dump(my_model, open(filename, 'wb'))\n",
    "pickle.dump(best_param_model, open(filename, 'wb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['accuracy']\n",
    "val_loss = history.history['val_accuracy']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training accuracy')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "\n",
    "value = model.predict(X_test)\n",
    "y_pred = np.argmax(value, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true,y_pred))\n",
    "\n",
    "dictionary = ['ANGER', 'CONTEMPT', 'DISGUST', 'FEAR', 'HAPPINESS',  'NEUTRAL', 'SADNESS', 'SURPRISE']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dictionary)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "disp.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'model_relu_best'\n",
    "path = cwd + \"/model/\" + name\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path)\n",
    "os.makedirs(path)\n",
    "\n",
    "model.save(path)\n",
    "# model = keras.models.load_model(path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
