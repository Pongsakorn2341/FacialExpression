{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import os\n",
    "import pandas as pd, numpy as np\n",
    "import cv2, imutils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from hyperopt.pyll.base import scope \n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "df = pd.read_csv(cwd + \"/augmentation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, math, numpy as np, dlib\n",
    "from imutils import face_utils\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(cwd + \"/../predictor/shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "\n",
    "pixels =  []\n",
    "labels = []\n",
    "for i, row in df.iterrows():\n",
    "    image_path = cwd + \"/augmentation/\" + row['image']\n",
    "    label = row['emotion']\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # detect face in grayscale image\n",
    "    rect = detector(image, 1)\n",
    "    \n",
    "    if len(rect) == 0:\n",
    "        error.append(i)\n",
    "        continue\n",
    "\n",
    "    shape = predictor(image, rect[0])\n",
    "    np_shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "    x = list(j[0].item() for j in np_shape)\n",
    "    y = list(j[1].item() for j in np_shape)\n",
    "\n",
    "    # np_shape = np.array(np_shape).flatten()\n",
    "    # roi_points.append(np_shape)\n",
    "\n",
    "    try:\n",
    "        # crop image\n",
    "        image = image[int(min(y)):int(max(y)), int(min(x)):int(max(x))]\n",
    "\n",
    "        # resize\n",
    "        image = cv2.resize(image, (112, 112), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        pixel = image.flatten()\n",
    "        pixels.append(pixel)\n",
    "        labels.append(label)\n",
    "\n",
    "    except:\n",
    "        error.append(i)\n",
    "        continue\n",
    "\n",
    "    if i > 0 and i % 10000 == 0:\n",
    "        print('[INFO] processed {}/{}'.format(i, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(error, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(cwd + \"/crop_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9432/3493430049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/crop_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(cwd + \"/crop_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels =  []\n",
    "labels = []\n",
    "for i, row in df.iterrows():\n",
    "    image_path = cwd + \"/augmentation/\" + row['image']\n",
    "    label = row['emotion']\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # detect face in grayscale image\n",
    "    rect = detector(image, 1)\n",
    "    \n",
    "    if len(rect) == 0:\n",
    "        error.append(i)\n",
    "        continue\n",
    "\n",
    "    shape = predictor(image, rect[0])\n",
    "    np_shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "    x = list(j[0].item() for j in np_shape)\n",
    "    y = list(j[1].item() for j in np_shape)\n",
    "\n",
    "    # crop image\n",
    "    image = image[int(min(y)):int(max(y)), int(min(x)):int(max(x))]\n",
    "\n",
    "    # resize\n",
    "    image = cv2.resize(image, (112, 112), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    pixel = image.flatten()\n",
    "    pixels.append(pixel)\n",
    "    labels.append(label)\n",
    "\n",
    "    if i > 0 and i % 10000 == 0:\n",
    "        print('[INFO] processed {}/{}'.format(i, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(pixels), np.array(labels), test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_val = le.transform(y_val)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    'rate'       : hp.uniform('rate', 0.01, 0.5),\n",
    "    'dropout'    : hp.uniform('dropout', 0.01, 0.5),\n",
    "    'units1'      : scope.int(hp.quniform('units1', 50, 100, 5)),\n",
    "    'units2'      : scope.int(hp.quniform('units2', 10, 100, 5)),\n",
    "    'units3'      : scope.int(hp.quniform('units3', 10, 50, 5)),\n",
    "    'units4'      : scope.int(hp.quniform('units4', 10, 50, 5)),\n",
    "    'batch_size' : scope.int(hp.quniform('batch_size', 100, 250, 25)),\n",
    "    'layers'     : scope.int(hp.quniform('layers', 3, 4, 1)),\n",
    "    'optimizer'  : hp.choice('optimizer', ['adam', 'adadelta', 'sgd', 'RMSprop']),\n",
    "    'epochs'     : scope.int(hp.quniform('epochs', 100, 300, 10)),\n",
    "    'activation' : hp.choice('activation', ['relu']), # ['relu', 'sigmoid', 'tanh', 'elu']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_nn(params):\n",
    "    print(\"params\", params)\n",
    "\n",
    "    # Keras LSTM model\n",
    "    model = Sequential()\n",
    "\n",
    "    # params['layers'] => num of hidden layer\n",
    "\n",
    "    if params['layers'] == 1:\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "    else:\n",
    "        # First layer specifies input_shape and returns sequences\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Middle layers return sequences\n",
    "        for i in range(params['layers']-2):\n",
    "            model.add(Dense(params['units' + str(i + 2)], activation=params['activation']))\n",
    "            #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Last layer doesn't return anything                                            \n",
    "        model.add(Dense(params['units' + str(params['layers'])], activation=params['activation']))\n",
    "        model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.compile(optimizer=params['optimizer'], loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss',mode='min', verbose=1,patience=15)\n",
    "    result =  model.fit(X_train, y_train, validation_data=(X_val, y_val,), batch_size=params['batch_size'], epochs=int(X_train.shape[0]/params['batch_size']), verbose=0)\n",
    "\n",
    "    # Get the lowest validation loss of the training epochs\n",
    "    validation_loss = np.amin(result.history['val_loss']) \n",
    "    print('Best validation loss of epoch:', validation_loss)\n",
    "\n",
    "    return {'loss': validation_loss, \n",
    "            'status': STATUS_OK, \n",
    "            'model': model, \n",
    "            'params': params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Hyperparameter\n",
    "trials = Trials()\n",
    "best = fmin(f_nn, \n",
    "            space, \n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
    "best_params = trials.results[np.argmin([r['loss'] for r in trials.results])]['params']\n",
    "worst_model = trials.results[np.argmax([r['loss'] for r in trials.results])]['model']\n",
    "worst_params = trials.results[np.argmax([r['loss'] for r in trials.results])]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation = 'relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation=activation, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation=activation))\n",
    "    model.add(Dense(16, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(lb.classes_), activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_best_param(params):\n",
    "    model = Sequential()\n",
    "\n",
    "    if params['layers'] == 1:\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "    else:\n",
    "        # First layer specifies input_shape and returns sequences\n",
    "        model.add(Dense(params['units1'], activation=params['activation'], input_shape=(X_train.shape[1],)))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Middle layers return sequences\n",
    "        for i in range(params['layers']-2):\n",
    "            model.add(Dense(params['units' + str(i + 2)], activation=params['activation']))\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "            #model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "        # Last layer doesn't return anything                                            \n",
    "        model.add(Dense(params['units' + str(params['layers'])], activation=params['activation']))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(Dropout(rate=params['rate']))\n",
    "\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.compile(optimizer=params['optimizer'], loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'activation': 'relu', 'batch_size': 225, 'dropout': 0.43494195542218583, 'epochs': 210, 'layers': 3, 'optimizer': 'adadelta', 'rate': 0.3152384932786021, 'units1': 100, 'units2': 100, 'units3': 45, 'units4': 80}\n",
    "\n",
    "model = create_model_best_param(best_params)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val,), batch_size=best_params['batch_size'], epochs=int(X_train.shape[0]/best_params['batch_size']), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'y', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "dictionary = {0: 'ANGER', 1: 'CONTEMPT', 2: 'DISGUST', 3: 'FEAR', 4: 'HAPPINESS',  5: 'NEUTRAL', 6: 'SADNESS', 7: 'SURPRISE'}\n",
    "target_name = [dictionary[i] for i in range(len(dictionary))]\n",
    "\n",
    "value = model.predict(X_test)\n",
    "y_pred = np.argmax(value, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true,y_pred, target_names=target_name, digits=4))\n",
    "\n",
    "dictionary = ['ANGER', 'CONTEMPT', 'DISGUST', 'FEAR', 'HAPPINESS',  'NEUTRAL', 'SADNESS', 'SURPRISE']\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dictionary)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "disp.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "dictionary = ['ANGER', 'CONTEMPT', 'DISGUST', 'FEAR', 'HAPPINESS',  'NEUTRAL', 'SADNESS', 'SURPRISE']\n",
    "\n",
    "for idx in range(len(dictionary)):\n",
    "    print(dictionary[idx])\n",
    "\n",
    "    emo_feature = np.copy(X_train)\n",
    "    emo_target = list(label[idx] for label in y_train)\n",
    "    emo_target = np.array(emo_target)\n",
    "\n",
    "    X = emo_feature\n",
    "    y = emo_target\n",
    "\n",
    "    history = []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # plot ROC curves\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    for i, (train, val) in enumerate(cv.split(X, y)):\n",
    "        X_train_kf, X_val_kf = X[train], X[val]\n",
    "        y_train_kf, y_val_kf = y[train], y[val]\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        y_train_kf = to_categorical(y_train_kf)\n",
    "        y_val_kf = to_categorical(y_val_kf)\n",
    "\n",
    "        model = create_model_best_param(best_params, 2)\n",
    "        model.fit(X_train_kf,\n",
    "                    y_train_kf,\n",
    "                    # validation_data=(X_val_kf, y_val_kf,),\n",
    "                    batch_size=best_params['batch_size'],\n",
    "                    epochs=best_params[\"epochs\"],\n",
    "                    verbose=2)\n",
    "\n",
    "        # predict\n",
    "        y_pred = model.predict(X_val_kf).ravel()\n",
    "        y_val_kf = y_val_kf.ravel()\n",
    "\n",
    "        print('====================Fold ', i , '====================')\n",
    "\n",
    "        # plot ROC curve\n",
    "        viz = RocCurveDisplay.from_predictions(y_val_kf, y_pred, ax=ax, name=\"ROC fold {}\".format(i), alpha=0.3, lw=1,)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    # middle line\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    # mean line\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    # std\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(\n",
    "        mean_fpr,\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05],\n",
    "            ylim=[-0.05, 1.05],\n",
    "            title=\"Receiver operating characteristic\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.savefig(cwd + '/../graph/' + dictionary[idx] + '/ann_relu.jpg')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efc329eb76fa8c61d98fc33828834e5a32dbc6c00e0ecf46855e3ac0d7163765"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
